{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update to your own data set directory, which contains the data sets given in the paper"
    "dataset_directory = \"/home/ole_fredrik_berg/datasets/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Hacker News comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hacker_news_data = pd.read_csv(dataset_directory + \"hacker_news_sample.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>dead</th>\n",
       "      <th>by</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>parent</th>\n",
       "      <th>descendants</th>\n",
       "      <th>ranking</th>\n",
       "      <th>deleted</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&amp;gt;&lt;i&gt;which leads me to say why are you using...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coldtea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.390844e+09</td>\n",
       "      <td>comment</td>\n",
       "      <td>7131680</td>\n",
       "      <td>7127578.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-01-27T17:31:13Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I would like to point out some counter-example...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>etanol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.319396e+09</td>\n",
       "      <td>comment</td>\n",
       "      <td>3146879</td>\n",
       "      <td>3145330.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-10-23T18:46:40Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.456641e+09</td>\n",
       "      <td>comment</td>\n",
       "      <td>11190089</td>\n",
       "      <td>11189361.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>2016-02-28T06:26:56Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;i&gt;Our msbuild implementation can now build Pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Locke1689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.407882e+09</td>\n",
       "      <td>comment</td>\n",
       "      <td>8170491</td>\n",
       "      <td>8170071.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-08-12T22:13:10Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No matter how awful iPhoto is it's still bette...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>miloshadzic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.362573e+09</td>\n",
       "      <td>comment</td>\n",
       "      <td>5330773</td>\n",
       "      <td>5327590.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-03-06T12:28:02Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title  url                                               text dead  \\\n",
       "0   NaN  NaN  &gt;<i>which leads me to say why are you using...  NaN   \n",
       "1   NaN  NaN  I would like to point out some counter-example...  NaN   \n",
       "2   NaN  NaN                                                NaN  NaN   \n",
       "3   NaN  NaN  <i>Our msbuild implementation can now build Pr...  NaN   \n",
       "4   NaN  NaN  No matter how awful iPhoto is it's still bette...  NaN   \n",
       "\n",
       "            by  score          time     type        id      parent  \\\n",
       "0      coldtea    NaN  1.390844e+09  comment   7131680   7127578.0   \n",
       "1       etanol    NaN  1.319396e+09  comment   3146879   3145330.0   \n",
       "2          NaN    NaN  1.456641e+09  comment  11190089  11189361.0   \n",
       "3    Locke1689    NaN  1.407882e+09  comment   8170491   8170071.0   \n",
       "4  miloshadzic    NaN  1.362573e+09  comment   5330773   5327590.0   \n",
       "\n",
       "   descendants  ranking deleted             timestamp  \n",
       "0          NaN      NaN     NaN  2014-01-27T17:31:13Z  \n",
       "1          NaN      NaN     NaN  2011-10-23T18:46:40Z  \n",
       "2          NaN      NaN    True  2016-02-28T06:26:56Z  \n",
       "3          NaN      NaN     NaN  2014-08-12T22:13:10Z  \n",
       "4          NaN      NaN     NaN  2013-03-06T12:28:02Z  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hacker_news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I would like to point out some counter-examples:<p>¬´<i>Think of journalists. Many are losing their jobs. Newspapers are barely surviving. In the old days for every news event there were probably hundreds of journalists writing about the same story for their own local newspapers. Now because of the efficiency of the Internet and search engines a few journalists writing about it would suffice for the whole country. People would be able to find them. There is no reason why hundreds of newspapers should write and publish their own versions of the same story.</i>¬ª<p>And yet you can still find incompetent people.  I've read so many inaccuracies grammar and typographic mistakes from information <i>professionals</i> (apparently) that makes me wonder if the price to pay for such efficiency is too high.  And I'm talking as a consumer information consumer.<p>¬´<i>Corporations are increasingly getting bigger (in terms of market caps) more global and more powerful yet they are getting smaller and smaller in terms of the number of people they employ because they have mastered the art of efficiency.</i>¬ª<p>Okay take IKEA for example.  IKEA sells furniture and other home accessories world wide and everywhere you can find the same model.  However how many people do they employ on each store?  A hundred?  I don't know but I'm under the impression that is not a shrinking number.  And IKEA is a particularly good example because it's expansion model is not based on a franchise like fast food restaurants.<p>Of course I might not understood the intention of the author.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hacker_news_data[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hacker_news_texts = hacker_news_data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /opt/conda/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.50.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.3.1)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;1m‚úò Couldn't link model to 'en'\u001b[0m\n",
      "Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "permissions and try re-running the command as admin, or use a virtualenv. You\n",
      "can still import the model as a module and call its load() method, or create the\n",
      "symlink manually.\n",
      "/opt/conda/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/data/en\n",
      "\u001b[38;5;3m‚ö† Download successful but linking failed\u001b[0m\n",
      "Creating a shortcut link for 'en' didn't work (maybe you don't have admin\n",
      "permissions?), but you can still load the model via its full package name: nlp =\n",
      "spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The preprocessing of the comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from emoji import UNICODE_EMOJI\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from functools import cache\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer_back = WordNetLemmatizer()\n",
    "allowed_letters = set(string.ascii_lowercase + \" -\") | set(UNICODE_EMOJI.keys())\n",
    "allowed_letters2 = set(string.ascii_lowercase + \" \") | set(UNICODE_EMOJI.keys())\n",
    "emojis = set(UNICODE_EMOJI.keys())\n",
    "\n",
    "memo = {}\n",
    "\n",
    "# Lemmatizing\n",
    "def lemmatizer(w):\n",
    "    if w in memo:\n",
    "        return memo[w]\n",
    "    r = lemmatizer_back.lemmatize(w, get_wordnet_pos(w))\n",
    "    if len(memo) < 50000:\n",
    "        memo[w] = r\n",
    "    return r\n",
    "\n",
    "#lemmatizer = cache(maxsize=50000)(my_lemmatizer)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"<i>\", \" \")\n",
    "    text = text.replace(\"\\\\n\", \" \")\n",
    "    text = text.replace(\"<p>\", \" \")\n",
    "    text = text.replace(\"</i>\", \" \")\n",
    "    text = text.replace(\"</p>\", \" \")\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \"-url-\", text)\n",
    "    text = re.sub(r\"\\&.{0,4}\\;\", \"\", text)\n",
    "    text = \"\".join([\" -emoji- \" if s in emojis else s for s in text if s in allowed_letters])\n",
    "    text = \"\".join([text[i] for i in range(len(text)) if (text[i] in allowed_letters2 or (text[i+1:i+5] == \"url-\" or text[i+1:i+7] == \"emoji-\" or text[i-4:i] == \"-url\" or text[i-6:i] == \"-emoji\"))])\n",
    "    text = re.sub(r\"\\ +\", \" \", text)\n",
    "    text = text.strip()\n",
    "    text = \" \".join([w if (\"-emoji-\" in w or \"-url-\" in w) else lemmatizer(w) for w in nltk.word_tokenize(text)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hacker_news_texts = [text for text in hacker_news_texts if type(text)==str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['&gt;<i>which leads me to say why are you using C to do X?</i><p>Because they know C it&#x27;s fast and it has lots of libs available. They might also dislike Java or CL.<p>Not every engineering decision is perfect lots of factors play in.<p>&gt;<i>Attempts to combine the best of C (speed) with the best of scripting languages (easy to do things fast without having to pay attention to what you are doing) in my opinion end up merely joining the worst of both worlds rather than the best of both worlds.</i><p>The &quot;pay attention&quot; things is to needless complexity (memory management etc). They only reason we put up with those things was to get speed. If we can get adequate speed without those nobody cares about them.<p>&gt;<i>Besides isn&#x27;t programming about being specific? Do you really want to code stuff without having to worry about the details?</i><p>No programming is about getting results. Nobody cares about the details in the level of programming language minutuae.<p>We care about the &quot;effort put in&quot; and &quot;quality&#x2F;speed of results coming out&quot; ratio.',\n",
       " \"I would like to point out some counter-examples:<p>¬´<i>Think of journalists. Many are losing their jobs. Newspapers are barely surviving. In the old days for every news event there were probably hundreds of journalists writing about the same story for their own local newspapers. Now because of the efficiency of the Internet and search engines a few journalists writing about it would suffice for the whole country. People would be able to find them. There is no reason why hundreds of newspapers should write and publish their own versions of the same story.</i>¬ª<p>And yet you can still find incompetent people.  I've read so many inaccuracies grammar and typographic mistakes from information <i>professionals</i> (apparently) that makes me wonder if the price to pay for such efficiency is too high.  And I'm talking as a consumer information consumer.<p>¬´<i>Corporations are increasingly getting bigger (in terms of market caps) more global and more powerful yet they are getting smaller and smaller in terms of the number of people they employ because they have mastered the art of efficiency.</i>¬ª<p>Okay take IKEA for example.  IKEA sells furniture and other home accessories world wide and everywhere you can find the same model.  However how many people do they employ on each store?  A hundred?  I don't know but I'm under the impression that is not a shrinking number.  And IKEA is a particularly good example because it's expansion model is not based on a franchise like fast food restaurants.<p>Of course I might not understood the intention of the author.\",\n",
       " '<i>Our msbuild implementation can now build Project K and Roslyn</i><p>Wow. Really impressive -- our MSBuild hackery is gut-wrenching.',\n",
       " \"No matter how awful iPhoto is it's still better than almost anything you can find on Linux. I know because that was one of my biggest gripes while I was using Ubuntu up to about 5 months ago.<p>And it's not about how Macs just work it's that you have apps for almost anything that actually work.\",\n",
       " \"The existence of a way to shard searches doesn't make scaling real time search on email (hint: do some back of the envelope calculation on how much data that involves) a non-issue.\",\n",
       " 'The actual Internet of things is Tesla collecting 130 million miles of autopilot data to make autopilot safer. It&#x27;s GE collecting data from the jet engines they produce to understand failures and do predictive maintenance. It&#x27;s Netapp collecting error logs from fileservers in the field so they know how to prioritize their bug database which performance bottlenecks to fix and what limitations actual customers encounter.<p>That&#x27;s the real internet of things. Just ignore all the (hype for) idiotic connected home crap covered by the popular press. Yes vendors are producing these products but the only feature I want is a a global disable for any such thing I accidentally bring into my home.',\n",
       " 'I want to know how ants got into a sealed bag of brown sugar when I can&#x27;t.',\n",
       " 'French is supremely broken. It makes no fucking sense as it&#x27;s influences are too varied. Which makes it great for poetry perhaps but not much else.',\n",
       " 'I actually went from Things to OmniFocus and am currently using a somewhat hacked collection of Todoist Asana &amp; IFTTT.',\n",
       " \"First impression after spending some time on try.discourse.org --- it's very messy.<p>The mess that online forums have been seemingly since forever should be reduced not continued with snazzier implementation.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hacker_news_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2984974\n"
     ]
    }
   ],
   "source": [
    "print(len(hacker_news_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hacker_news_texts_formatted = [preprocess(text) for text in hacker_news_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Hacker News data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"formated_data/hacker_news.pickle\", 'wb') as save_file:\n",
    "    pickle.dump(hacker_news_texts_formatted, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and processing Youtube data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_us_data = open(dataset_directory + \"UScomments.csv\", 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_gb_data = open(dataset_directory + \"GBcomments.csv\", 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_us_data = youtube_us_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_gb_data = youtube_gb_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_us_data = [s[s.find('\"')+1:s.rfind('\"')-1] for s in youtube_us_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_us_data = youtube_us_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_gb_data = [s[s.find('\"')+1:s.rfind('\"')-1] for s in youtube_gb_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_gb_data = youtube_gb_data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "youtube_data = youtube_gb_data + youtube_us_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's more accurate to call it the M+ (1000) because the price is closer than calling it the X (10)\",\n",
       " 'To be there with a samsung phone\\\\nüòÇüòÇ',\n",
       " 'Thank gosh, a place I can watch it without having to be at HD... my speed doesn‚Äôt support H',\n",
       " 'What happened to the home button on the iPhone X? *****Cough****copying Samsung******coug',\n",
       " 'Power is the disease.\\xa0 Care is the cure.\\xa0 Keep caring for yourself and others as best as you can.\\xa0 This is life',\n",
       " 'Keep calm and buy iphone 8 Keep calm and buy iphone 8 plus Keep calm and buy iphone X. What is your favourite',\n",
       " 'i am a big fan of youtube and u !!!!!!!!!!!!',\n",
       " 'You will never find Losers who line up and pay good money for this crap line up to do charity or community work',\n",
       " \"*APPLE JUST COMMENTED ON MY LAST VIDEO* I'm crying right now üò≠üò≠üò≠üò¢üò¢\",\n",
       " \"I'm only here to see Emma, I love her so much! i'm so proud of her! :\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#youtube_data_formatted = [preprocess(text) for text in youtube_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it more accurate to call it the m because the price be closer than call it the x',\n",
       " 'to be there with a samsung phone -emoji- -emoji-',\n",
       " 'thank gosh a place i can watch it without have to be at hd my speed doesnt support h',\n",
       " 'what happen to the home button on the iphone x coughcopying samsungcoug',\n",
       " 'power be the disease care be the cure keep care for yourself and others a best a you can this be life',\n",
       " 'keep calm and buy iphone keep calm and buy iphone plus keep calm and buy iphone x what be your favourite',\n",
       " 'i be a big fan of youtube and u',\n",
       " 'you will never find loser who line up and pay good money for this crap line up to do charity or community work',\n",
       " 'apple just comment on my last video im cry right now -emoji- -emoji- -emoji- -emoji- -emoji-',\n",
       " 'im only here to see emma i love her so much im so proud of her',\n",
       " 'mom say just one more video before be',\n",
       " 'what be the song',\n",
       " 'i start cry at when emmas ep show up -emoji- -emoji- im so proud of he',\n",
       " 'year late on wireless charge year late on oled technology old facial recognition tech enhance by old ir tech so your late on just about every frontwhat to do i know let lose what make our product instantly recognizable this be apple window samsung be bound to be love this',\n",
       " 'more one before to be',\n",
       " 'subscribe me if you love your mumm',\n",
       " 'they really just take the samsung s and name it iphone x or whateve',\n",
       " 'so if someone have a twin sibling both can unlock iphone x haha',\n",
       " 'for god sake it well than my pspr',\n",
       " 'mp cam what kind of standard be this in',\n",
       " 'he be wait for people to -emoji- cla',\n",
       " 'sarahah have a leak in it system revealhaters',\n",
       " 'if apple suck and samsungs well like this commen',\n",
       " 'this video be too long halfway through it i get hungry so i left it play and go to the kitchen to fix my self a sandwich but then i found out that im out of mayonnaise so i go to a store there i saw the most beautiful woman i have ever see in my whole life but im really a shy person so i take up a threeyear personality development course so i can introduce my self she be very friendly and all but unfortunately she have a boyfriend so i say all good im a mature person i want the best for her and i harbor no illusion that i be the best person for her and she seem happy with her boyfriend so i do not bother her anymore but we kept in touch and we become friend and i get over my crush on her then she broke up with her boyfriend we drank some alcohol because of it i told her shell be fine and i wish her well i still think shes the most beautiful woman in the world but like i say i be over my crush on her it be like five year already when i first saw her besides i be quiet happy with the friendship i developed with her it be more important than a crush so we kept hang out drinking have coffee and all i have a girlfriend she start date other guy my girlfriend want to live some other life without me in it so i say okay i want the best for you and i want you to pursue your happiness my lady friend and i drank alcohol about it and she give me the same advice i give her when she be in that position and i become okay with the breakup immediately but we be really drunk so she spent the night in my apartment i only have one bed so you know what that mean she take the bed and i slept on the couch but on the couch i really cant sleep something be bother me so i toss and turn for about three hour then i finally cant take it anymore i stood up and go straight to my room where shes sleep i approach the bed gently sat on it and i reach for her shoulder to pull her closer to me she stir and woke up she ask whats up i told her you know the first time i saw you i be watch a video and left it play to get my self a sandwich then go to the store to get some mayo then i get distract by life that i forgot to finish the video she say you know what ive be wonder about a weird noise in your night drawer so we open that drawer and lo and behold there my phone and this video still have two minute of play time on it',\n",
       " 'you kick steve job out the company and do video with the name from it',\n",
       " 'love be all you need yeah love and a shiny new iphone',\n",
       " 'the best thing to ever happen to iphon',\n",
       " 'how to internet speed video -emoji- -emoji- -emoji- -emoji- -emoji- -emoji- -emoji- -emoji- -emoji- -emoji- -url-',\n",
       " 'sub to my channe',\n",
       " 'which song be tha']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_data_formatted[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving processed Youtube comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"formated_data/youtube.pickle\", 'wb') as save_file:\n",
    "    pickle.dump(youtube_data_formatted, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the lemmatizer for using on the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"formated_data/lemmatizer.json\", 'w') as save_file:\n",
    "    save_file.write(json.dumps(memo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Reddit comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"/home/ole_fredrik_berg/datasets/reddit.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = conn.execute(\"SELECT body FROM May2015 limit 2000000\")\n",
    "texts = [r[0] for r in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['„Åè„Åù\\nË™≠„Åø„Åü„ÅÑ„ÅåË≤∑„Å£„Åü„ÇâË≤†„Åë„Å™Ê∞ó„Åå„Åô„Çã\\nÂõ≥Êõ∏È§®„Å´Âá∫„Å≠„Éº„Åã„Å™',\n",
       " \"gg this one's over. off to watch the NFL draft I guess\",\n",
       " \"Are you really implying we return to those times or anywhere near that political environment?  If so, you won't have much luck selling the American people on that governance concept without ushering in American Revolution 2.0.\",\n",
       " \"No one has a European accent either  because it doesn't exist. There are accents from Europe but not a European accent.\",\n",
       " 'That the kid \"..reminds me of Kevin.\"   so sad :-(',\n",
       " 'Haha, i was getting nauseous from it, if that was your ingame experience that would have given a whole new level of Bloodborne ^^ ',\n",
       " \"After reading this, I wholeheartedly believe you should let her go. \\n\\nYou and her simply aren't compatible. She's looking for a committment and you're bent on avoiding it. You should figure out your committment issues before getting into a committed relationship.  \",\n",
       " \"Let's do this. See you guys on the other side.\",\n",
       " 'You can buy a mystery sampler from small batch and request them',\n",
       " \"Nihilum and LG are significantly better off in theory. I can't really think of a replacement for Ptr that would leave CLG in a better place than they were before. Cloud9 should be much better, but you never know.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and saving Reddit comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }],
   "source": [
    "for i in range(5):\n",
    "    res = conn.execute(\"SELECT body FROM May2015 limit 2000000 offset \"+str(2000000*i))\n",
    "    texts = [r[0] for r in res]\n",
    "    formatted_reddit_comments = [preprocess(t) for t in texts]\n",
    "    formatted_reddit_comments = [c for c in formatted_reddit_comments if c != \"\"]\n",
    "    with open(\"formated_data/reddit\"+str(i)+\".pickle\", 'wb') as save_file:\n",
    "        pickle.dump(formatted_reddit_comments, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
